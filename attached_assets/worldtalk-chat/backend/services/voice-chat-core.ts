/**
 * VoiceChatCore - è¯­éŸ³èŠå¤©å¼•æ“
 * 
 * æ ¸å¿ƒåŠŸèƒ½ï¼š
 * 1. è¯­éŸ³è¾“å…¥ -> ASRè½¬å†™ -> ä¿ç•™åŸå§‹è¯­éŸ³ + è½¬å†™æ–‡æœ¬
 * 2. æ”¯æŒå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«
 * 3. ä¸ºæ•°å­—äººä¼šè¯æä¾›è¯­éŸ³èŠå¤©èƒ½åŠ›
 */

import { speechToText } from './dashscope-speech';
import { recognizeAudioNonStreaming } from './batch-stt';
import { downloadFromOSS } from './oss';
import fs from 'fs';
import path from 'path';
import os from 'os';

// æ”¯æŒçš„è¯­éŸ³è¯­è¨€ä»£ç 
export type VoiceLangCode = "zh" | "en" | "th" | "vi" | "fr" | "ja" | "ko" | "auto";

// è¯­è¨€ä»£ç æ˜ å°„ï¼ˆç”¨äºASRï¼‰
const LANG_CODE_MAP: Record<VoiceLangCode, string[]> = {
  "zh": ["zh"],
  "en": ["en"],
  "th": ["th"],
  "vi": ["vi"],
  "fr": ["fr"],
  "ja": ["ja"],
  "ko": ["ko"],
  "auto": ["zh", "en", "th", "ja", "ko"],
};

// è¾“å…¥æ¨¡å¼
export type InputMode = "text" | "voice_input" | "voice_chat";

// è¯­éŸ³èŠå¤©é…ç½®
export interface VoiceChatConfig {
  enabledLanguages: VoiceLangCode[];
}

// æ•°å­—äººä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
export interface AgentInfo {
  id: string;
  name: string;
  capabilities: string[];
  voiceChatConfig?: VoiceChatConfig;
}

// VoiceChatCore è¾“å…¥
export interface VoiceChatInput {
  sessionId: string;
  agent: AgentInfo;
  userId: string;
  mode: InputMode;
  text?: string;
  audioUrl?: string;  // OSS URL
  audioBuffer?: Buffer;  // æˆ–ç›´æ¥ä¼  Buffer
}

// VoiceChatCore è¾“å‡º
export interface VoiceChatOutput {
  text: string;           // è½¬å†™/åŸå§‹æ–‡æœ¬
  rawAudioUrl?: string;   // åŸå§‹è¯­éŸ³ URL
  transcript?: string;    // è½¬å†™æ–‡æœ¬ï¼ˆç”¨äºé•¿æŒ‰æ˜¾ç¤ºï¼‰
  audioDuration?: number; // è¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
  detectedLanguage?: string;
}

/**
 * ä» OSS URL ä¸‹è½½éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°
 */
async function downloadAudioFromUrl(audioUrl: string): Promise<{ localPath: string; isTemp: boolean }> {
  if (audioUrl.startsWith('https://') || audioUrl.startsWith('http://')) {
    const urlObj = new URL(audioUrl);
    const ossPath = urlObj.pathname.substring(1);
    const localPath = await downloadFromOSS(ossPath);
    return { localPath, isTemp: true };
  }
  return { localPath: audioUrl, isTemp: false };
}

/**
 * ä¼°ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆç®€å•ç‰ˆæœ¬ï¼ŒåŸºäºæ–‡ä»¶å¤§å°ï¼‰
 * å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨ ffprobe è·å–ç²¾ç¡®æ—¶é•¿
 */
async function estimateAudioDuration(audioPath: string): Promise<number> {
  try {
    const stats = fs.statSync(audioPath);
    // å‡è®¾å¹³å‡æ¯”ç‰¹ç‡ 32kbps for voice
    const durationSeconds = Math.round(stats.size / (32 * 1024 / 8));
    return Math.max(1, Math.min(durationSeconds, 300)); // 1ç§’ - 5åˆ†é’Ÿ
  } catch {
    return 0;
  }
}

/**
 * é€‰æ‹©ä½¿ç”¨çš„è¯­éŸ³è¯­è¨€
 * ä¼˜å…ˆä½¿ç”¨æ•°å­—äººé…ç½®çš„è¯­è¨€ï¼Œå¦åˆ™ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
 */
function pickVoiceLanguage(agent: AgentInfo, _userId: string): VoiceLangCode {
  const config = agent.voiceChatConfig;
  if (config?.enabledLanguages && config.enabledLanguages.length > 0) {
    return config.enabledLanguages[0];
  }
  return "auto";
}

/**
 * VoiceChatCore ä¸»å‡½æ•°
 * å¤„ç†è¯­éŸ³è¾“å…¥ï¼Œè¿”å›è½¬å†™æ–‡æœ¬å’ŒåŸå§‹è¯­éŸ³ä¿¡æ¯
 */
export async function voiceChatCore(input: VoiceChatInput): Promise<VoiceChatOutput> {
  const { sessionId, agent, userId, mode, text, audioUrl, audioBuffer } = input;

  console.log(`ğŸ¤ [VoiceChatCore] å¼€å§‹å¤„ç†: mode=${mode}, hasAudioUrl=${!!audioUrl}, hasBuffer=${!!audioBuffer}`);

  // 1) æ–‡æœ¬æ¨¡å¼ï¼šç›´æ¥è¿”å›
  if (mode === "text" && text) {
    return {
      text: text,
      rawAudioUrl: undefined,
      transcript: undefined,
    };
  }

  // 2) è¯­éŸ³æ¨¡å¼ï¼šéœ€è¦ ASR è½¬å†™
  if ((mode === "voice_input" || mode === "voice_chat") && (audioUrl || audioBuffer)) {
    let userText = text;
    let duration = 0;
    let detectedLang = "unknown";
    
    try {
      // é€‰æ‹©è¯­è¨€
      const lang = pickVoiceLanguage(agent, userId);
      console.log(`ğŸŒ [VoiceChatCore] ä½¿ç”¨è¯­è¨€: ${lang}`);

      if (audioBuffer) {
        // ç›´æ¥ä½¿ç”¨ Buffer è¿›è¡Œè¯†åˆ«
        userText = await recognizeAudioNonStreaming(audioBuffer, 'webm');
        duration = Math.round(audioBuffer.length / (32 * 1024 / 8));
      } else if (audioUrl) {
        // ä» URL ä¸‹è½½å¹¶è¯†åˆ«
        const { localPath, isTemp } = await downloadAudioFromUrl(audioUrl);
        
        try {
          const result = await speechToText(localPath, lang);
          userText = result.text;
          detectedLang = result.language;
          duration = await estimateAudioDuration(localPath);
        } finally {
          if (isTemp) {
            try { fs.unlinkSync(localPath); } catch {}
          }
        }
      }

      console.log(`âœ… [VoiceChatCore] è½¬å†™å®Œæˆ: "${userText?.substring(0, 50)}..." (${duration}s)`);

    } catch (error) {
      console.error(`âŒ [VoiceChatCore] è¯­éŸ³è¯†åˆ«å¤±è´¥:`, error);
      userText = text || "[è¯­éŸ³è¯†åˆ«å¤±è´¥]";
    }

    return {
      text: userText || "",
      rawAudioUrl: audioUrl,
      transcript: userText,
      audioDuration: duration,
      detectedLanguage: detectedLang,
    };
  }

  // 3) å›é€€ï¼šè¿”å›åŸå§‹æ–‡æœ¬
  return {
    text: text || "",
    rawAudioUrl: audioUrl,
    transcript: undefined,
  };
}

/**
 * æ£€æŸ¥æ•°å­—äººæ˜¯å¦å…·å¤‡è¯­éŸ³èŠå¤©èƒ½åŠ›
 */
export function hasVoiceChatCapability(agent: AgentInfo): boolean {
  return agent.capabilities?.includes("voice_chat") || false;
}

/**
 * è·å–æ•°å­—äººå·²å¼€é€šçš„è¯­éŸ³è¯­è¨€åˆ—è¡¨
 */
export function getEnabledVoiceLanguages(agent: AgentInfo): VoiceLangCode[] {
  return agent.voiceChatConfig?.enabledLanguages || [];
}

/**
 * æ‰€æœ‰æ”¯æŒçš„è¯­éŸ³è¯­è¨€åˆ—è¡¨
 */
export function getAllSupportedVoiceLangs(): VoiceLangCode[] {
  return ["zh", "en", "th", "vi", "fr", "ja", "ko"];
}
