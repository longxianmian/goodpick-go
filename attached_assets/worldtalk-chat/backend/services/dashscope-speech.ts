import FormData from 'form-data';
import fs from 'fs';
import { downloadFromOSS } from './oss';
import { getDashScopeApiKey } from '../config/dashscope';

// ä½¿ç”¨ä¸­å›½ç‰ˆendpointï¼ˆå›½é™…ç‰ˆä¸º dashscope-intl.aliyuncs.comï¼‰
const DASHSCOPE_API_BASE = 'https://dashscope.aliyuncs.com/api/v1';

/**
 * è¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰- å°†éŸ³é¢‘è½¬æ–‡å­—
 * ä½¿ç”¨ Paraformer-v2 å®æ—¶è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰
 * @param audioInput - å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€URLï¼Œæˆ–ç›´æ¥ä¼ å…¥ Buffer
 */
export async function speechToText(audioInput: string | Buffer, language: string = 'auto'): Promise<{
  text: string;
  language: string;
}> {
  const apiKey = getDashScopeApiKey();
  console.log('[dashscope] speechToText DASHSCOPE_API_KEY prefix:', apiKey.slice(0, 8));

  let audioBase64: string;
  let isTemporaryFile = false;
  let localFilePath: string | null = null;

  try {
    // ğŸš€ ç›´æ¥ä¼ å…¥ Buffer - æœ€å¿«è·¯å¾„ï¼Œè·³è¿‡æ‰€æœ‰æ–‡ä»¶æ“ä½œ
    if (Buffer.isBuffer(audioInput)) {
      audioBase64 = audioInput.toString('base64');
      console.log(`âš¡ [STT] ç›´æ¥ä½¿ç”¨Buffer (${audioInput.length} bytes)`);
    }
    // å¦‚æœæ˜¯OSS URLï¼Œå…ˆä¸‹è½½åˆ°æœ¬åœ°
    else if (audioInput.startsWith('https://') || audioInput.startsWith('http://')) {
      const urlObj = new URL(audioInput);
      const ossPath = urlObj.pathname.substring(1);
      localFilePath = await downloadFromOSS(ossPath);
      isTemporaryFile = true;
      const audioBuffer = fs.readFileSync(localFilePath);
      audioBase64 = audioBuffer.toString('base64');
    }
    // æœ¬åœ°æ–‡ä»¶è·¯å¾„
    else {
      const audioBuffer = fs.readFileSync(audioInput);
      audioBase64 = audioBuffer.toString('base64');
    }
    
    const startTime = Date.now();

    // ä½¿ç”¨ Paraformer-v2 å®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆæ›´å¿«ï¼‰
    const response = await fetch(`${DASHSCOPE_API_BASE}/services/audio/asr/transcription`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'paraformer-v2',
        input: {
          audio: audioBase64,
          format: 'webm',
          sample_rate: 16000
        },
        parameters: {
          language_hints: language === 'auto' ? ['zh', 'en', 'th', 'ja', 'ko'] : [language]
        }
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      // å¦‚æœ paraformer-v2 å¤±è´¥ï¼Œå›é€€åˆ° qwen3-asr-flash
      console.warn(`Paraformer-v2 failed, falling back to qwen3-asr-flash: ${errorText}`);
      return await speechToTextFallback(audioBase64, language);
    }

    const result = await response.json();
    const elapsed = Date.now() - startTime;
    console.log(`âš¡ STTå®Œæˆ (${elapsed}ms)`);

    // æå–è¯†åˆ«æ–‡æœ¬
    const transcript = result.output?.text || result.output?.sentence || '';
    const detectedLanguage = result.output?.language || language;

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if (isTemporaryFile && localFilePath) {
      try { fs.unlinkSync(localFilePath); } catch {}
    }

    return { text: transcript, language: detectedLanguage };
  } catch (error) {
    if (isTemporaryFile && localFilePath) {
      try { fs.unlinkSync(localFilePath); } catch {}
    }
    console.error('è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
    throw error;
  }
}

/**
 * å¤‡ç”¨STTæ–¹æ³• - ä½¿ç”¨ qwen3-asr-flash
 */
async function speechToTextFallback(audioBase64: string, language: string): Promise<{
  text: string;
  language: string;
}> {
  const apiKey = getDashScopeApiKey();
  const response = await fetch(`${DASHSCOPE_API_BASE}/services/aigc/multimodal-generation/generation`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'qwen3-asr-flash',
      input: {
        messages: [{
          role: 'user',
          content: [{
            audio: `data:audio/webm;base64,${audioBase64}`
          }]
        }]
      },
      parameters: {
        asr_options: {
          enable_itn: true,
          language: language === 'auto' ? undefined : language
        }
      }
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`DashScope ASR failed: ${response.status} - ${errorText}`);
  }

  const result = await response.json();
  const transcript = result.output?.choices?.[0]?.message?.content?.[0]?.text || '';
  const detectedLanguage = result.output?.asr_result?.language || language;

  return { text: transcript, language: detectedLanguage };
}

/**
 * è¯­éŸ³åˆæˆï¼ˆTTSï¼‰- å°†æ–‡å­—è½¬è¯­éŸ³
 * ä½¿ç”¨ Qwen TTS æ¨¡å‹
 */
export async function textToSpeech(
  text: string,
  targetLanguage: string = 'Chinese',
  voice?: string
): Promise<{
  audioUrl: string;
  audioBase64?: string;
}> {
  const apiKey = getDashScopeApiKey();
  console.log('[dashscope] textToSpeech DASHSCOPE_API_KEY prefix:', apiKey.slice(0, 8));

  try {
    // æ ¹æ®è¯­è¨€é€‰æ‹©é»˜è®¤å£°éŸ³
    const defaultVoice = getDefaultVoiceForLanguage(targetLanguage);
    const selectedVoice = voice || defaultVoice;

    // è°ƒç”¨DashScope TTS API
    const response = await fetch(`${DASHSCOPE_API_BASE}/services/aigc/multimodal-generation/generation`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'qwen3-tts-flash',
        input: {
          text: text,
          voice: selectedVoice,
          language_type: targetLanguage
        },
        parameters: {
          format: 'mp3',
          sample_rate: 16000
        }
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`DashScope TTS failed: ${response.status} - ${errorText}`);
    }

    const result = await response.json();

    // æ£€æŸ¥å¤šç§å¯èƒ½çš„å“åº”æ ¼å¼
    const audioBase64 = result.output?.audio?.data || result.output?.audio || result.data;
    const audioUrl = result.output?.audio?.url || result.output?.url || result.url;
    
    if (!audioBase64 && !audioUrl) {
      throw new Error(`No audio data in TTS response. Response: ${JSON.stringify(result)}`);
    }
    
    // å¦‚æœæœ‰URLï¼Œä¸‹è½½å¹¶ä¸Šä¼ åˆ°OSSï¼ˆDashScopeçš„URLä¼šè¿‡æœŸï¼‰
    let buffer: Buffer;
    if (audioUrl) {
      const audioResponse = await fetch(audioUrl);
      if (!audioResponse.ok) {
        throw new Error(`Failed to download audio from DashScope: ${audioResponse.status}`);
      }
      const arrayBuffer = await audioResponse.arrayBuffer();
      buffer = Buffer.from(arrayBuffer);
    } else if (audioBase64) {
      // å°†base64è§£ç ä¸ºBuffer
      buffer = Buffer.from(audioBase64 as string, 'base64');
    } else {
      throw new Error('No audio data available');
    }
    
    // ä¸Šä¼ åˆ°é˜¿é‡Œäº‘OSSï¼ˆæ°¸ä¹…å­˜å‚¨ï¼‰
    const { nanoid } = await import('nanoid');
    const { uploadToOSS, generateSignedUrl } = await import('./oss');
    
    const fileName = `tts-${Date.now()}-${nanoid(8)}.mp3`;
    const { ossPath } = await uploadToOSS(buffer, 'voices', fileName);
    
    // ç”Ÿæˆç­¾åURLï¼ˆ30å¤©æœ‰æ•ˆæœŸï¼‰
    const ossAudioUrl = await generateSignedUrl(ossPath, 2592000);

    return {
      audioUrl: ossAudioUrl,
      audioBase64: audioBase64 as string
    };
  } catch (error) {
    console.error('è¯­éŸ³åˆæˆå¤±è´¥:', error);
    throw error;
  }
}

/**
 * æ ¹æ®è¯­è¨€é€‰æ‹©é»˜è®¤å£°éŸ³
 */
function getDefaultVoiceForLanguage(language: string): string {
  const voiceMap: Record<string, string> = {
    'Chinese': 'Sunny',
    'zh': 'Sunny',
    'English': 'Cherry',
    'en': 'Cherry',
    'Japanese': 'Kiki',
    'ja': 'Kiki',
    'Thai': 'Cherry',  // æ³°è¯­å¯ä»¥ç”¨è‹±è¯­å£°éŸ³
    'th': 'Cherry',
    'Indonesian': 'Cherry', // å°å°¼è¯­å¯ä»¥ç”¨è‹±è¯­å£°éŸ³
    'id': 'Cherry',
    'Auto': 'Cherry'
  };

  return voiceMap[language] || 'Cherry';
}

/**
 * å°†ç”¨æˆ·è®¾ç½®çš„è¯­éŸ³åå¥½æ˜ å°„åˆ° DashScope TTS æ”¯æŒçš„éŸ³è‰²
 * 
 * DashScope Qwen3-TTS-Flash æ”¯æŒçš„éŸ³è‰² (2024):
 * - Cherry: å¥³å£°ï¼ˆä¸­è‹±åŒè¯­ï¼‰
 * - Ethan: ç”·å£°
 * - Elias: ç”·å£°
 * - Jada: å¥³å£°ï¼ˆä¸Šæµ·è¯ï¼‰
 * - Dylan: ç”·å£°ï¼ˆåŒ—äº¬è¯ï¼‰
 * - Sunny: å¥³å£°ï¼ˆå››å·è¯ï¼‰
 * - Rocky: ç”·å£°
 * 
 * ç”¨æˆ·è®¾ç½®é€‰é¡¹:
 * - default: ä½¿ç”¨ç³»ç»Ÿé»˜è®¤
 * - male: ç”·å£°
 * - female: å¥³å£°
 * - male_deep: ç”·å£°(æµ‘åš)
 * - female_sweet: å¥³å£°(ç”œç¾)
 * - neutral: ä¸­æ€§
 */
export function mapUserVoicePreference(preference: string | undefined, language: string): string {
  if (!preference || preference === 'default') {
    return getDefaultVoiceForLanguage(language);
  }
  
  // DashScope Qwen3-TTS æ”¯æŒçš„éŸ³è‰²æ˜ å°„
  // ç”·å£°: Ethan(æ ‡å‡†), Dylan(åŒ—äº¬è¯), Elias, Rocky
  // å¥³å£°: Cherry(æ ‡å‡†), Sunny(å››å·è¯), Jada(ä¸Šæµ·è¯)
  const voiceMapping: Record<string, string> = {
    'male': 'Ethan',           // æ ‡å‡†ç”·å£°
    'female': 'Cherry',        // æ ‡å‡†å¥³å£°
    'male_deep': 'Rocky',      // æµ‘åšç”·å£°
    'female_sweet': 'Jada',    // ç”œç¾å¥³å£°
    'neutral': 'Cherry',       // ä¸­æ€§ -> å¥³å£°
  };
  
  const mappedVoice = voiceMapping[preference];
  if (mappedVoice) {
    return mappedVoice;
  }
  
  // å¦‚æœç”¨æˆ·ç›´æ¥ä¼ å…¥äº† DashScope æ”¯æŒçš„éŸ³è‰²åç§°ï¼Œç›´æ¥ä½¿ç”¨
  const validVoices = ['Cherry', 'Ethan', 'Elias', 'Jada', 'Dylan', 'Sunny', 'Rocky'];
  if (validVoices.includes(preference)) {
    return preference;
  }
  
  return getDefaultVoiceForLanguage(language);
}

/**
 * è¯­è¨€ä»£ç æ˜ å°„ - DashScopeä½¿ç”¨çš„è¯­è¨€ä»£ç 
 */
export function mapLanguageToDashScope(lang: string): string {
  const langMap: Record<string, string> = {
    'zh': 'Chinese',
    'en': 'English',
    'ja': 'Japanese',
    'th': 'English',  // æ³°è¯­ä½¿ç”¨è‹±è¯­å£°éŸ³
    'id': 'English',  // å°å°¼è¯­ä½¿ç”¨è‹±è¯­å£°éŸ³
  };

  return langMap[lang] || 'Auto';
}
