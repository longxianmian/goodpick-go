import { useState, useRef, useCallback, useEffect } from 'react';
import { useSimpleRealtimeContext } from '@/contexts/simple-realtime-context';

interface RealtimeSTTOptions {
  onInterimResult?: (text: string) => void;
  onFinalResult?: (text: string) => void;
  onError?: (error: string) => void;
  onReady?: () => void;
  onClosed?: () => void;
}

// æ¨¡å—çº§å•ä¾‹ AudioContextï¼Œå¤ç”¨ä»¥å‡å°‘åˆå§‹åŒ–å¼€é”€
let sharedAudioContext: AudioContext | null = null;

function getSharedAudioContext(): AudioContext {
  if (!sharedAudioContext || sharedAudioContext.state === 'closed') {
    sharedAudioContext = new AudioContext({ sampleRate: 16000 });
  }
  return sharedAudioContext;
}

// ç¡®ä¿ AudioContext å¤„äºæ´»è·ƒçŠ¶æ€ï¼ˆiOS Safari éœ€è¦ç”¨æˆ·äº¤äº’åæ‰èƒ½ resumeï¼‰
async function ensureAudioContextResumed(): Promise<void> {
  const ctx = getSharedAudioContext();
  if (ctx.state === 'suspended') {
    await ctx.resume();
  }
}

const SILENCE_TIMEOUT_MS = 6000; // 6ç§’æ— è¯­éŸ³è‡ªåŠ¨å…³é—­

export function useRealtimeSTT(options: RealtimeSTTOptions = {}) {
  const [isRecording, setIsRecording] = useState(false);
  const [isReady, setIsReady] = useState(false);
  
  const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | ScriptProcessorNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const sessionIdRef = useRef<string | null>(null);
  const optionsRef = useRef(options);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const stopRecordingRef = useRef<(() => void) | null>(null);
  optionsRef.current = options;
  
  const { sendMessage, isConnected, addMessageHandler, removeMessageHandler } = useSimpleRealtimeContext();

  // é‡ç½®é™éŸ³è®¡æ—¶å™¨
  const resetSilenceTimer = useCallback(() => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }
    silenceTimerRef.current = setTimeout(() => {
      console.log('â±ï¸ 6ç§’æ— è¯­éŸ³è¾“å…¥ï¼Œè‡ªåŠ¨å…³é—­');
      if (stopRecordingRef.current) {
        stopRecordingRef.current();
      }
    }, SILENCE_TIMEOUT_MS);
  }, []);

  // æ¸…é™¤é™éŸ³è®¡æ—¶å™¨
  const clearSilenceTimer = useCallback(() => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
  }, []);

  useEffect(() => {
    const handleSTTMessage = (msg: any) => {
      switch (msg.type) {
        case 'stt-ready':
          console.log('ğŸ¤ STTå°±ç»ª');
          sessionIdRef.current = msg.sessionId;
          setIsReady(true);
          optionsRef.current.onReady?.();
          break;
          
        case 'stt-transcript':
          if (msg.text) {
            // æ”¶åˆ°è¯­éŸ³ç»“æœï¼Œé‡ç½®é™éŸ³è®¡æ—¶å™¨
            resetSilenceTimer();
            
            if (msg.isFinal) {
              console.log(`ğŸ“ æœ€ç»ˆç»“æœ: "${msg.text.substring(0, 50)}..."`);
              optionsRef.current.onFinalResult?.(msg.text);
            } else {
              console.log(`ğŸ“ å®æ—¶: "${msg.text.substring(0, 30)}..."`);
              optionsRef.current.onInterimResult?.(msg.text);
            }
          }
          break;
          
        case 'stt-error':
          console.error('âŒ STTé”™è¯¯:', msg.error);
          optionsRef.current.onError?.(msg.error);
          break;
          
        case 'stt-closed':
          console.log('ğŸ”Œ STTä¼šè¯å…³é—­');
          setIsReady(false);
          sessionIdRef.current = null;
          clearSilenceTimer();
          optionsRef.current.onClosed?.();
          break;
      }
    };

    addMessageHandler(handleSTTMessage);
    return () => removeMessageHandler(handleSTTMessage);
  }, [addMessageHandler, removeMessageHandler, resetSilenceTimer, clearSilenceTimer]);

  const startRecording = useCallback(async () => {
    if (!isConnected) {
      optionsRef.current.onError?.('WebSocketæœªè¿æ¥');
      return;
    }

    try {
      // ç¡®ä¿ AudioContext å¤„äºæ´»è·ƒçŠ¶æ€
      await ensureAudioContextResumed();
      const audioContext = getSharedAudioContext();
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true
        } 
      });
      streamRef.current = stream;

      sendMessage({ type: 'stt-start' });
      
      const source = audioContext.createMediaStreamSource(stream);
      sourceNodeRef.current = source;
      
      // ä½¿ç”¨ ScriptProcessorNodeï¼ŒbufferSize=2048 (128ms @ 16kHz)
      // æ›´å¤§çš„ç¼“å†²åŒºå‡å°‘ CPU å¼€é”€ï¼ŒåŒæ—¶ä¿æŒåˆç†çš„å»¶è¿Ÿ
      const processor = audioContext.createScriptProcessor(2048, 1, 1);
      workletNodeRef.current = processor;
      
      let chunkCount = 0;
      processor.onaudioprocess = (e) => {
        if (!sessionIdRef.current) return;
        
        const inputData = e.inputBuffer.getChannelData(0);
        const pcm16 = new Int16Array(inputData.length);
        
        for (let i = 0; i < inputData.length; i++) {
          const s = Math.max(-1, Math.min(1, inputData[i]));
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        
        const base64 = btoa(
          new Uint8Array(pcm16.buffer).reduce(
            (data, byte) => data + String.fromCharCode(byte),
            ''
          )
        );
        
        sendMessage({ 
          type: 'stt-audio', 
          audio: base64,
          sessionId: sessionIdRef.current
        });
        
        chunkCount++;
        if (chunkCount % 8 === 0) {
          console.log(`ğŸ¤ å·²å‘é€ ${chunkCount} å— (${(chunkCount * 128).toFixed(0)}ms)`);
        }
      };
      
      source.connect(processor);
      processor.connect(audioContext.destination);
      
      setIsRecording(true);
      // å¼€å§‹å½•éŸ³æ—¶å¯åŠ¨é™éŸ³è®¡æ—¶å™¨
      resetSilenceTimer();
      console.log('ğŸ¤ å¼€å§‹è¯­éŸ³è¾“å…¥ (PCM 16kHz, 128ms/å—, 6ç§’æ— è¯­éŸ³è‡ªåŠ¨å…³é—­)');
    } catch (error: any) {
      console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', error);
      optionsRef.current.onError?.(error.message || 'æ— æ³•è®¿é—®éº¦å…‹é£');
    }
  }, [isConnected, sendMessage, resetSilenceTimer]);

  const stopRecording = useCallback(() => {
    // æ¸…é™¤é™éŸ³è®¡æ—¶å™¨
    clearSilenceTimer();
    
    // æ–­å¼€éŸ³é¢‘èŠ‚ç‚¹ï¼ˆä½†ä¸å…³é—­å…±äº«çš„ AudioContextï¼‰
    if (workletNodeRef.current) {
      workletNodeRef.current.disconnect();
      workletNodeRef.current = null;
    }
    
    if (sourceNodeRef.current) {
      sourceNodeRef.current.disconnect();
      sourceNodeRef.current = null;
    }

    // åœæ­¢éº¦å…‹é£æµ
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    if (sessionIdRef.current) {
      sendMessage({ type: 'stt-stop', sessionId: sessionIdRef.current });
    }

    setIsRecording(false);
    setIsReady(false);
    console.log('ğŸ›‘ åœæ­¢è¯­éŸ³è¾“å…¥');
  }, [sendMessage, clearSilenceTimer]);

  // å°† stopRecording å­˜å‚¨åˆ° ref ä¸­ï¼Œä»¥ä¾¿é™éŸ³è®¡æ—¶å™¨å¯ä»¥è°ƒç”¨
  useEffect(() => {
    stopRecordingRef.current = stopRecording;
  }, [stopRecording]);

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†è®¡æ—¶å™¨
  useEffect(() => {
    return () => {
      clearSilenceTimer();
    };
  }, [clearSilenceTimer]);

  return {
    isRecording,
    isReady,
    startRecording,
    stopRecording,
    isConnected
  };
}
